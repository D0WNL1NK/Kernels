{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bd95d4ec3521>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;31m# print(np.array(list(dataset[20].nodes())).min())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdslist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdslist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datafile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdslist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataset_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[0my_mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, splitext\n",
    "import networkx as nx\n",
    "import sys\n",
    "sys.path.append('/')\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split, ParameterGrid\n",
    "from gklearn.utils.kernels import gaussiankernel, polynomialkernel\n",
    "from gklearn.utils.graphdataset import get_dataset_attributes\n",
    "from gklearn.utils.graph_files import load_dataset\n",
    "from gklearn.utils.parallel import parallel_gm\n",
    "from gklearn.utils import Dataset\n",
    "from gklearn.utils.graphfiles import loadDataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import time as tm\n",
    "\n",
    "dslist = [{'name': 'Alkane', 'datafile': 'datasets/Alkane/dataset.ds','dataset_y': 'datasets/Alkane/dataset_boiling_point_names.txt', 'task':'regression'},\n",
    "          {'name': 'Acyclic', 'datafile': 'datasets/acyclic/dataset_bps.ds','dataset_y': None,'task':'regression'},\n",
    "          {'name': 'MAO', 'datafile': 'datasets/MAO/dataset.ds', 'dataset_y': None,'task':'classification'},\n",
    "          {'name': 'PAH', 'datafile': 'datasets/PAH/dataset.ds', 'dataset_y': None,'task':'classification'}, # unlabeled\n",
    "          {'name': 'MUTAG', 'datafile': 'datasets/MUTAG/MUTAG_A.txt', 'dataset_y': None,'task':'classification'}, # node/edge symb\n",
    "           {'name': 'Letter-med', 'datafile': 'datasets/Letter-med/Letter-med_A.txt', 'dataset_y': None,'task':'classification'},\n",
    "    # node nsymb\n",
    "          {'name': 'ENZYMES', 'datafile': 'datasets/ENZYMES_txt/ENZYMES_A_sparse.txt', 'dataset_y': None,'task':'regression'}]\n",
    "#           {'name': 'Fingerprint', 'datafile': 'datasets/Fingerprint/Fingerprint_A.txt','dataset_y': None}]\n",
    "\n",
    "# To calculate the clustering coefficient\n",
    "def calccluster(G):\n",
    "    listcluster = []\n",
    "    for node in G.nodes():\n",
    "        neighbours=[n for n in nx.neighbors(G,node)]\n",
    "        n_neighbors=len(neighbours)\n",
    "        n_links=0\n",
    "        if n_neighbors>1:\n",
    "            for node1 in neighbours:\n",
    "                for node2 in neighbours:\n",
    "                    if G.has_edge(node1,node2) == True:\n",
    "                        n_links=n_links+1\n",
    "            n_links/=2.0 #because n_links is calculated twice\n",
    "            clustering_coefficient=n_links/(0.5*n_neighbors*(n_neighbors-1))\n",
    "            listcluster.append(clustering_coefficient)\n",
    "        else:\n",
    "            listcluster.append(0)\n",
    "    return listcluster\n",
    "\n",
    "# To calculate the objective function\n",
    "def calcimp(G, node_label, weight):\n",
    "    listimp = []\n",
    "    nodelist = np.array(list(G.nodes())) # Necessary to scale node value to array index for easy addressing of label array and centrality array\n",
    "    listcluster = calccluster(G)\n",
    "    x = list(nx.betweenness_centrality(G, weight=weight).values())\n",
    "    for node in G.nodes():\n",
    "        neighbours=[n for n in nx.neighbors(G,node)]\n",
    "        listlabel = list(nx.get_node_attributes(G, node_label).values())\n",
    "#         print(\"node is \", node)\n",
    "        cnt=0\n",
    "        n_neighbours=len(neighbours)\n",
    "        if n_neighbours == 0:\n",
    "            n_neighbours = 1\n",
    "        for node1 in neighbours:\n",
    "            if listlabel[node-nodelist.min()] != listlabel[node1-nodelist.min()]:\n",
    "                cnt=cnt+1\n",
    "        listimp.append(-0.33*((cnt/n_neighbours)+x[node-nodelist.min()]+listcluster[node-nodelist.min()]))\n",
    "    return listimp\n",
    "        \n",
    "    \n",
    "def chooseDataset(ds_name):\n",
    "    \"\"\"Choose dataset according to name.\n",
    "    \"\"\"\n",
    "    from gklearn.utils import Dataset\n",
    "    \n",
    "    dataset = Dataset()\n",
    "\n",
    "    # no node labels (and no edge labels).\n",
    "    if ds_name == 'Alkane':\n",
    "        dataset.load_predefined_dataset(ds_name)\n",
    "        dataset.trim_dataset(edge_required=False)\n",
    "        irrelevant_labels = {'node_attrs': ['x', 'y', 'z'], 'edge_labels': ['bond_stereo']}\n",
    "        dataset.remove_labels(**irrelevant_labels)\n",
    "    # node symbolic labels.\n",
    "    elif ds_name == 'Acyclic':\n",
    "        dataset.load_predefined_dataset(ds_name)\n",
    "        dataset.trim_dataset(edge_required=False)\n",
    "        irrelevant_labels = {'node_attrs': ['x', 'y', 'z'], 'edge_labels': ['bond_stereo']}\n",
    "        dataset.remove_labels(**irrelevant_labels)\n",
    "    # node non-symbolic labels.\n",
    "    elif ds_name == 'Letter-med':\n",
    "        dataset.load_predefined_dataset(ds_name)\n",
    "        dataset.trim_dataset(edge_required=False)\n",
    "    # node symbolic and non-symbolic labels (and edge symbolic labels).\n",
    "    elif ds_name == 'AIDS':\n",
    "        dataset.load_predefined_dataset(ds_name)\n",
    "        dataset.trim_dataset(edge_required=False)\n",
    "    # edge non-symbolic labels (no node labels).\n",
    "    elif ds_name == 'Fingerprint_edge':\n",
    "        dataset.load_predefined_dataset('Fingerprint')\n",
    "        dataset.trim_dataset(edge_required=True)\n",
    "        irrelevant_labels = {'edge_attrs': ['orient', 'angle']}\n",
    "        dataset.remove_labels(**irrelevant_labels)\n",
    "    # edge non-symbolic labels (and node non-symbolic labels).\n",
    "    elif ds_name == 'Fingerprint':\n",
    "        dataset.load_predefined_dataset(ds_name)\n",
    "        dataset.trim_dataset(edge_required=True)\n",
    "    # edge symbolic and non-symbolic labels (and node symbolic and non-symbolic labels).\n",
    "    elif ds_name == 'Cuneiform':\n",
    "        dataset.load_predefined_dataset(ds_name)\n",
    "        dataset.trim_dataset(edge_required=True)\n",
    "        \n",
    "    dataset.cut_graphs(range(0, 3))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def weisfeilerlehmankernel(*args, \n",
    "                           node_label='atom',\n",
    "                           edge_weight=None,\n",
    "                           edge_label='bond_type',\n",
    "                           height=0,\n",
    "                           base_kernel='subtree',\n",
    "                           parallel=None,\n",
    "                           n_jobs=None, \n",
    "                           verbose=True,\n",
    "                           numlandmarks = 2,\n",
    "                           sub_kernel=gaussiankernel):\n",
    "    \"\"\"Calculate Weisfeiler-Lehman kernels between graphs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Gn : List of NetworkX graph\n",
    "        List of graphs between which the kernels are calculated.\n",
    "    \n",
    "    G1, G2 : NetworkX graphs\n",
    "        Two graphs between which the kernel is calculated.      \n",
    "    node_label : string\n",
    "        Node attribute used as label. The default node label is atom.       \n",
    "    edge_label : string\n",
    "        Edge attribute used as label. The default edge label is bond_type.      \n",
    "    height : int\n",
    "        Subtree height.\n",
    "    base_kernel : string\n",
    "        Base kernel used in each iteration of WL kernel. Only default 'subtree' \n",
    "        kernel can be applied for now.\n",
    "    parallel : None\n",
    "        Which paralleliztion method is applied to compute the kernel. No \n",
    "        parallelization can be applied for now.\n",
    "    n_jobs : int\n",
    "        Number of jobs for parallelization. The default is to use all \n",
    "        computational cores. This argument is only valid when one of the \n",
    "        parallelization method is applied and can be ignored for now.\n",
    "    Return\n",
    "    ------\n",
    "    Kmatrix : Numpy matrix\n",
    "        Kernel matrix, each element of which is the Weisfeiler-Lehman kernel between 2 praphs.\n",
    "    Notes\n",
    "    -----\n",
    "    This function now supports WL subtree kernel only.\n",
    "    \"\"\"\n",
    "#       The default base \n",
    "#       kernel is subtree kernel. For user-defined kernel, base_kernel is the \n",
    "#       name of the base kernel function used in each iteration of WL kernel. \n",
    "#       This function returns a Numpy matrix, each element of which is the \n",
    "#       user-defined Weisfeiler-Lehman kernel between 2 praphs.\n",
    "#       pre-process\n",
    "\n",
    "    base_kernel = base_kernel.lower()\n",
    "    \n",
    "    Gn = args[0] if len(args) == 1 else [args[0], args[1]] # arrange all graphs in a list\n",
    "    Gn = [g.copy() for g in Gn]\n",
    "    \n",
    "    Kmatrix = np.zeros((len(Gn), len(Gn)))\n",
    "    weight = None\n",
    "    if edge_weight is None:\n",
    "        if verbose:\n",
    "            print('\\n None edge weight specified. Set all weight to 1.\\n')\n",
    "    else:\n",
    "        try:\n",
    "            some_weight = list(nx.get_edge_attributes(Gn[0], edge_weight).values())[0]\n",
    "            if isinstance(some_weight, (float, int)):\n",
    "                weight = edge_weight\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('\\n Edge weight with name %s is not float or integer. Set all weight to 1.\\n'% edge_weight)\n",
    "        except:\n",
    "            if verbose:\n",
    "                print('\\n Edge weight with name \"%s\" is not found in the edge attributes. Set all weight to 1.\\n'% edge_weight)\n",
    "    \n",
    "    ds_attrs = get_dataset_attributes(Gn, attr_names=['node_labeled', 'node_attr_dim', 'edge_labeled', 'edge_attr_dim', 'is_directed'], \n",
    "                                      node_label=node_label, edge_label=edge_label)\n",
    "\n",
    "    labeled = False\n",
    "    if ds_attrs['node_labeled'] or ds_attrs['edge_labeled']:\n",
    "        labeled = True\n",
    "        if not ds_attrs['node_labeled']:\n",
    "            for G in Gn:\n",
    "                nx.set_node_attributes(G, '0', 'atom')\n",
    "        if not ds_attrs['edge_labeled']:\n",
    "            for G in Gn:\n",
    "                nx.set_edge_attributes(G, '0', 'bond_type')\n",
    "\n",
    "    start_time = tm.time()\n",
    "\n",
    "    # for WL subtree kernel\n",
    "    if base_kernel == 'subtree':           \n",
    "        Gn, gn_landmarklist = _wl_kernel_do(Gn, node_label, weight, edge_label, height, parallel, n_jobs, verbose, numlandmarks)\n",
    "    \n",
    "    # get all canonical keys of all graphs before calculating kernels to save \n",
    "    # time, but this may cost a lot of memory for large dataset.\n",
    "    canonkeys = []\n",
    "    for g in (tqdm(Gn, desc='getting canonkeys', file=sys.stdout) if verbose else Gn):\n",
    "        canonkeys.append(get_canonkeys(g, node_label, edge_label, labeled, \n",
    "                                       ds_attrs['is_directed']))\n",
    "\n",
    "    # compute kernels.\n",
    "    from itertools import combinations_with_replacement\n",
    "    itr = combinations_with_replacement(range(0, len(Gn)), 2)\n",
    "    for i, j in (tqdm(itr, desc='getting canonkeys', file=sys.stdout) if verbose else itr):\n",
    "        Kmatrix[i][j] = _treeletkernel_do(canonkeys[i], canonkeys[j], sub_kernel)\n",
    "        Kmatrix[j][i] = Kmatrix[i][j] # @todo: no directed graph considered?\n",
    "        \n",
    "    run_time = tm.time() - start_time\n",
    "    if verbose:\n",
    "        print(\"\\n --- treelet kernel matrix of size %d built in %s seconds ---\"% (len(Gn), run_time))\n",
    "\n",
    "    return Kmatrix, run_time\n",
    "\n",
    "\n",
    "\n",
    "#<<<<<<<<<<<<<< THIS CALCULATES THE WL LABELS AND RETURNS A SUBGRAPH >>>>>>>>>>>>>>>>>>>#\n",
    "def _wl_kernel_do(Gn, node_label, weight, edge_label, height, parallel, n_jobs, verbose, numlandmarks):\n",
    "    \"\"\"Calculate Weisfeiler-Lehman kernels between graphs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Gn : List of NetworkX graph\n",
    "        List of graphs between which the kernels are calculated.       \n",
    "    node_label : string\n",
    "        node attribute used as label.\n",
    "    edge_label : string\n",
    "        edge attribute used as label.     \n",
    "    height : int\n",
    "        wl height.\n",
    "    Return\n",
    "    ------\n",
    "    Kmatrix : Numpy matrix\n",
    "        Kernel matrix, each element of which is the Weisfeiler-Lehman kernel between 2 praphs.\n",
    "    \"\"\"\n",
    "    height = int(height)\n",
    "    Kmatrix = np.zeros((len(Gn), len(Gn)))\n",
    "    \n",
    "    # initial for height = 0\n",
    "    all_num_of_each_label = [] # number of occurence of each label in each graph in this iteration\n",
    "    \n",
    "    all_labels_ori = [] # Initial lsit of all labels\n",
    "    \n",
    "    # for each graph\n",
    "    for G in Gn:\n",
    "        # get the set of original labels\n",
    "        labels_ori = list(nx.get_node_attributes(G, node_label).values())\n",
    "        # number of occurence of each label in G\n",
    "        all_num_of_each_label.append(dict(Counter(labels_ori)))\n",
    "        all_labels_ori.append(labels_ori)\n",
    "#         print(\"Clustering coefficients are:\", type(nx.clustering(G)))\n",
    "#     print(\"Original labels are: \\n\", all_labels_ori)\n",
    "#     print(\"All numbers of each labels are: \\n\", all_num_of_each_label)\n",
    "    # calculate subtree kernel with the 0th iteration and add it to the final kernel\n",
    "#     compute_kernel_matrix(Kmatrix, all_num_of_each_label, Gn, parallel, n_jobs, False)\n",
    "\n",
    "    # iterate each height\n",
    "    for h in range(1, height + 1):\n",
    "        print(\"<<<<<<<<< ITERATION\", h, \">>>>>>>>>>>\")\n",
    "        all_set_compressed = {} # a dictionary mapping original labels to new ones in all graphs in this iteration\n",
    "        num_of_labels_occured = 0 # number of the set of letters that occur before as node labels at least once in all graphs\n",
    "#       all_labels_ori = set() # all unique orignal labels in all graphs in this iteration\n",
    "        all_num_of_each_label = [] # number of occurence of each label in G\n",
    "\n",
    "\n",
    "        for idx, G in enumerate(Gn):\n",
    "\n",
    "            all_multisets = []\n",
    "            for node, attrs in G.nodes(data=True):\n",
    "                # Multiset-label determination.\n",
    "                multiset = [G.nodes[neighbors][node_label] for neighbors in G[node]]\n",
    "                # sorting each multiset\n",
    "                multiset.sort()\n",
    "                multiset = [attrs[node_label]] + multiset # add the prefix \n",
    "                all_multisets.append(tuple(multiset))\n",
    "#             print(\"All multisets are:\", all_multisets)\n",
    "            # label compression\n",
    "            set_unique = list(set(all_multisets)) # set of unique multiset labels\n",
    "            # a dictionary mapping original labels to new ones. \n",
    "            set_compressed = {}\n",
    "            # if a label occured before, assign its former compressed label, \n",
    "            # else assign the number of labels occured + 1 as the compressed label. \n",
    "            for value in set_unique:\n",
    "                if value in all_set_compressed.keys():\n",
    "                    set_compressed.update({value: all_set_compressed[value]})\n",
    "                else:\n",
    "                    set_compressed.update({value: str(num_of_labels_occured + 1)})\n",
    "                    num_of_labels_occured += 1\n",
    "\n",
    "            all_set_compressed.update(set_compressed)\n",
    "\n",
    "            # relabel nodes\n",
    "            for idx, node in enumerate(G.nodes()):\n",
    "                G.nodes[node][node_label] = set_compressed[all_multisets[idx]]\n",
    "            \n",
    "#             print(\"Changed Nodes are\", list(nx.get_node_attributes(G,node_label).values()))\n",
    "#             print(\"New clustering coefficient is\", nx.clustering(G))\n",
    "            # get the set of compressed labels\n",
    "            labels_comp = list(nx.get_node_attributes(G, node_label).values())\n",
    "#             print(\"compressed labels are: \", labels_comp)\n",
    "#           all_labels_ori.update(labels_comp)\n",
    "            all_num_of_each_label.append(dict(Counter(labels_comp)))\n",
    "    \n",
    "    # Compute the value for every node of every graph\n",
    "    listcomp = [] # list of computed values, has the vertices instead\n",
    "    listval = [] # list of lists having actual values\n",
    "    for G in Gn:\n",
    "        x = calcimp(G, node_label, weight)\n",
    "        listval.append(x)\n",
    "        y = np.array(x)\n",
    "        z = np.argsort(y)\n",
    "        listcomp.append(z)\n",
    "    # Now select those vertices having max value of function. Remember, include atleast one vertex of each label\n",
    "#     print(\"SORTED NODES ARE HERE YOOOOOOOOOOOOO:\\n\", listcomp)\n",
    "#     print(\"SORTED VALUES ARE HERE YOOOOOOOOOOOO:\\n\",listval)\n",
    "    print(\"<<<<<<<<<<<<<<<<< TIME TO FIND LANDMARKS YOOOOO >>>>>>>>>>>>>>>>\")\n",
    "    gn_landmarklist = [] # List of landmarks of all graphs\n",
    "    for i in range(len(listcomp)):\n",
    "        size = len(all_labels_ori[i])\n",
    "        num_landmarks = numlandmarks if numlandmarks < size else size\n",
    "        set_uniq = set(all_labels_ori[i])\n",
    "        list_uniq = list(set_uniq)\n",
    "        uniqsize = len(list_uniq)\n",
    "        uniqcount = np.zeros(uniqsize)\n",
    "        gi_landmarklist = []\n",
    "        currlandmarks = 0 # Number of landmarks extracted\n",
    "        for j in range(len(listcomp[i])):\n",
    "            for k in range(uniqsize):\n",
    "                if currlandmarks < num_landmarks:\n",
    "                    if ((all_labels_ori[i][listcomp[i][j]] == list_uniq[k]) and (uniqcount[k] == 0)):\n",
    "                        uniqcount[k] = uniqcount[k] + 1\n",
    "                        gi_landmarklist.append(listcomp[i][j])\n",
    "                        currlandmarks=currlandmarks+1\n",
    "                else:\n",
    "                    break\n",
    "                        \n",
    "        if currlandmarks < num_landmarks:\n",
    "            for x in range(len(listcomp[i])):\n",
    "                if currlandmarks < num_landmarks:\n",
    "                    if (listcomp[i][x] in gi_landmarklist):\n",
    "                        continue\n",
    "                    else:\n",
    "                        gi_landmarklist.append(listcomp[i][x])\n",
    "                        currlandmarks=currlandmarks+1\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "        gn_landmarklist.append(gi_landmarklist)     \n",
    "#     print(\"<<<<<<<<<<<<<<<<<< FINAL LIST OF LANDMARKS ARE HERE >>>>>>>>>>>>>>>>>\\n\", gn_landmarklist)\n",
    "    \n",
    "    return extract_subgraphs(Gn, gn_landmarklist), gn_landmarklist\n",
    "        \n",
    "    \n",
    "\n",
    "def extract_subgraphs(Gn, gn_landmarklist):\n",
    "    Gn_new = []\n",
    "    for i in range(len(Gn)):\n",
    "        G1 = Gn[i].subgraph(gn_landmarklist[i]).copy()\n",
    "        Gn_new.append(G1)\n",
    "    return Gn_new\n",
    "    \n",
    "##################### CODE AS IT IS BEGINS ################################################################################\n",
    "\n",
    "def _treeletkernel_do(canonkey1, canonkey2, sub_kernel):\n",
    "    \"\"\"Calculate treelet graph kernel between 2 graphs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    canonkey1, canonkey2 : list\n",
    "        List of canonical keys in 2 graphs, where each key is represented by a string.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    kernel : float\n",
    "        Treelet Kernel between 2 graphs.\n",
    "    \"\"\"\n",
    "    keys = set(canonkey1.keys()) & set(canonkey2.keys()) # find same canonical keys in both graphs\n",
    "    vector1 = np.array([(canonkey1[key] if (key in canonkey1.keys()) else 0) for key in keys])\n",
    "    vector2 = np.array([(canonkey2[key] if (key in canonkey2.keys()) else 0) for key in keys]) \n",
    "    kernel = sub_kernel(vector1, vector2) \n",
    "    return kernel\n",
    "\n",
    "\n",
    "def wrapper_treeletkernel_do(sub_kernel, itr):\n",
    "    i = itr[0]\n",
    "    j = itr[1]\n",
    "    return i, j, _treeletkernel_do(G_canonkeys[i], G_canonkeys[j], sub_kernel)\n",
    "\n",
    "\n",
    "def get_canonkeys(G, node_label, edge_label, labeled, is_directed):\n",
    "    \"\"\"Generate canonical keys of all treelets in a graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G : NetworkX graphs\n",
    "        The graph in which keys are generated.\n",
    "    node_label : string\n",
    "        node attribute used as label. The default node label is atom.       \n",
    "    edge_label : string\n",
    "        edge attribute used as label. The default edge label is bond_type.\n",
    "    labeled : boolean\n",
    "        Whether the graphs are labeled. The default is True.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    canonkey/canonkey_l : dict\n",
    "        For unlabeled graphs, canonkey is a dictionary which records amount of \n",
    "        every tree pattern. For labeled graphs, canonkey_l is one which keeps \n",
    "        track of amount of every treelet.\n",
    "    \"\"\"\n",
    "    patterns = {} # a dictionary which consists of lists of patterns for all graphlet.\n",
    "    canonkey = {} # canonical key, a dictionary which records amount of every tree pattern.\n",
    "\n",
    "    ### structural analysis ###\n",
    "    ### In this section, a list of patterns is generated for each graphlet, \n",
    "    ### where every pattern is represented by nodes ordered by Morgan's \n",
    "    ### extended labeling.\n",
    "    # linear patterns\n",
    "    patterns['0'] = G.nodes()\n",
    "    canonkey['0'] = nx.number_of_nodes(G)\n",
    "    for i in range(1, 6): # for i in range(1, 6):\n",
    "        patterns[str(i)] = find_all_paths(G, i, is_directed)\n",
    "        canonkey[str(i)] = len(patterns[str(i)])\n",
    "\n",
    "    # n-star patterns\n",
    "    patterns['3star'] = [[node] + [neighbor for neighbor in G[node]] for node in G.nodes() if G.degree(node) == 3]\n",
    "    patterns['4star'] = [[node] + [neighbor for neighbor in G[node]] for node in G.nodes() if G.degree(node) == 4]\n",
    "    patterns['5star'] = [[node] + [neighbor for neighbor in G[node]] for node in G.nodes() if G.degree(node) == 5]      \n",
    "    # n-star patterns\n",
    "    canonkey['6'] = len(patterns['3star'])\n",
    "    canonkey['8'] = len(patterns['4star'])\n",
    "    canonkey['d'] = len(patterns['5star'])\n",
    "\n",
    "    # pattern 7\n",
    "    patterns['7'] = [] # the 1st line of Table 1 in Ref [1]\n",
    "    for pattern in patterns['3star']:\n",
    "        for i in range(1, len(pattern)): # for each neighbor of node 0\n",
    "            if G.degree(pattern[i]) >= 2:\n",
    "                pattern_t = pattern[:]\n",
    "                # set the node with degree >= 2 as the 4th node\n",
    "                pattern_t[i], pattern_t[3] = pattern_t[3], pattern_t[i]\n",
    "                for neighborx in G[pattern[i]]:\n",
    "                    if neighborx != pattern[0]:\n",
    "                        new_pattern = pattern_t + [neighborx]\n",
    "                        patterns['7'].append(new_pattern)\n",
    "    canonkey['7'] = len(patterns['7'])\n",
    "\n",
    "    # pattern 11\n",
    "    patterns['11'] = [] # the 4th line of Table 1 in Ref [1]\n",
    "    for pattern in patterns['4star']:\n",
    "        for i in range(1, len(pattern)):\n",
    "            if G.degree(pattern[i]) >= 2:\n",
    "                pattern_t = pattern[:]\n",
    "                pattern_t[i], pattern_t[4] = pattern_t[4], pattern_t[i]\n",
    "                for neighborx in G[pattern[i]]:\n",
    "                    if neighborx != pattern[0]:\n",
    "                        new_pattern = pattern_t + [ neighborx ]\n",
    "                        patterns['11'].append(new_pattern)\n",
    "    canonkey['b'] = len(patterns['11'])\n",
    "\n",
    "    # pattern 12\n",
    "    patterns['12'] = [] # the 5th line of Table 1 in Ref [1]\n",
    "    rootlist = [] # a list of root nodes, whose extended labels are 3\n",
    "    for pattern in patterns['3star']:\n",
    "        if pattern[0] not in rootlist: # prevent to count the same pattern twice from each of the two root nodes\n",
    "            rootlist.append(pattern[0])\n",
    "            for i in range(1, len(pattern)):\n",
    "                if G.degree(pattern[i]) >= 3:\n",
    "                    rootlist.append(pattern[i])\n",
    "                    pattern_t = pattern[:]\n",
    "                    pattern_t[i], pattern_t[3] = pattern_t[3], pattern_t[i]\n",
    "                    for neighborx1 in G[pattern[i]]:\n",
    "                        if neighborx1 != pattern[0]:\n",
    "                            for neighborx2 in G[pattern[i]]:\n",
    "                                if neighborx1 > neighborx2 and neighborx2 != pattern[0]:\n",
    "                                    new_pattern = pattern_t + [neighborx1] + [neighborx2]\n",
    "#                        new_patterns = [ pattern + [neighborx1] + [neighborx2] for neighborx1 in G[pattern[i]] if neighborx1 != pattern[0] for neighborx2 in G[pattern[i]] if (neighborx1 > neighborx2 and neighborx2 != pattern[0]) ]\n",
    "                                    patterns['12'].append(new_pattern)\n",
    "    canonkey['c'] = int(len(patterns['12']) / 2)\n",
    "\n",
    "    # pattern 9\n",
    "    patterns['9'] = [] # the 2nd line of Table 1 in Ref [1]\n",
    "    for pattern in patterns['3star']:\n",
    "        for pairs in [ [neighbor1, neighbor2] for neighbor1 in G[pattern[0]] if G.degree(neighbor1) >= 2 \\\n",
    "            for neighbor2 in G[pattern[0]] if G.degree(neighbor2) >= 2 if neighbor1 > neighbor2 ]:\n",
    "            pattern_t = pattern[:]\n",
    "            # move nodes with extended labels 4 to specific position to correspond to their children\n",
    "            pattern_t[pattern_t.index(pairs[0])], pattern_t[2] = pattern_t[2], pattern_t[pattern_t.index(pairs[0])]\n",
    "            pattern_t[pattern_t.index(pairs[1])], pattern_t[3] = pattern_t[3], pattern_t[pattern_t.index(pairs[1])]\n",
    "            for neighborx1 in G[pairs[0]]:\n",
    "                if neighborx1 != pattern[0]:\n",
    "                    for neighborx2 in G[pairs[1]]:\n",
    "                        if neighborx2 != pattern[0]:\n",
    "                            new_pattern = pattern_t + [neighborx1] + [neighborx2]\n",
    "                            patterns['9'].append(new_pattern)\n",
    "    canonkey['9'] = len(patterns['9'])\n",
    "\n",
    "    # pattern 10\n",
    "    patterns['10'] = [] # the 3rd line of Table 1 in Ref [1]\n",
    "    for pattern in patterns['3star']:       \n",
    "        for i in range(1, len(pattern)):\n",
    "            if G.degree(pattern[i]) >= 2:\n",
    "                for neighborx in G[pattern[i]]:\n",
    "                    if neighborx != pattern[0] and G.degree(neighborx) >= 2:\n",
    "                        pattern_t = pattern[:]\n",
    "                        pattern_t[i], pattern_t[3] = pattern_t[3], pattern_t[i]\n",
    "                        new_patterns = [ pattern_t + [neighborx] + [neighborxx] for neighborxx in G[neighborx] if neighborxx != pattern[i] ]\n",
    "                        patterns['10'].extend(new_patterns)\n",
    "    canonkey['a'] = len(patterns['10'])\n",
    "\n",
    "    ### labeling information ###\n",
    "    ### In this section, a list of canonical keys is generated for every \n",
    "    ### pattern obtained in the structural analysis section above, which is a \n",
    "    ### string corresponding to a unique treelet. A dictionary is built to keep\n",
    "    ### track of the amount of every treelet.\n",
    "    if labeled == True:\n",
    "        canonkey_l = {} # canonical key, a dictionary which keeps track of amount of every treelet.\n",
    "\n",
    "        # linear patterns\n",
    "        canonkey_t = Counter(list(nx.get_node_attributes(G, node_label).values()))\n",
    "        for key in canonkey_t:\n",
    "            canonkey_l[('0', key)] = canonkey_t[key]\n",
    "\n",
    "        for i in range(1, 6): # for i in range(1, 6):\n",
    "            treelet = []\n",
    "            for pattern in patterns[str(i)]:\n",
    "                canonlist = list(chain.from_iterable((G.nodes[node][node_label], \\\n",
    "                    G[node][pattern[idx+1]][edge_label]) for idx, node in enumerate(pattern[:-1])))\n",
    "                canonlist.append(G.nodes[pattern[-1]][node_label])\n",
    "                canonkey_t = canonlist if canonlist < canonlist[::-1] else canonlist[::-1]\n",
    "                treelet.append(tuple([str(i)] + canonkey_t))\n",
    "            canonkey_l.update(Counter(treelet))\n",
    "\n",
    "        # n-star patterns\n",
    "        for i in range(3, 6):\n",
    "            treelet = []\n",
    "            for pattern in patterns[str(i) + 'star']:\n",
    "                canonlist = [tuple((G.nodes[leaf][node_label], \n",
    "                                    G[leaf][pattern[0]][edge_label])) for leaf in pattern[1:]]\n",
    "                canonlist.sort()\n",
    "                canonlist = list(chain.from_iterable(canonlist))\n",
    "                canonkey_t = tuple(['d' if i == 5 else str(i * 2)] + [G.nodes[pattern[0]][node_label]] + canonlist)\n",
    "                treelet.append(canonkey_t)\n",
    "            canonkey_l.update(Counter(treelet))\n",
    "\n",
    "        # pattern 7\n",
    "        treelet = []\n",
    "        for pattern in patterns['7']:\n",
    "            canonlist = [tuple((G.nodes[leaf][node_label], \n",
    "                                G[leaf][pattern[0]][edge_label])) for leaf in pattern[1:3]]\n",
    "            canonlist.sort()\n",
    "            canonlist = list(chain.from_iterable(canonlist))\n",
    "            canonkey_t = tuple(['7'] + [G.nodes[pattern[0]][node_label]] + canonlist \n",
    "                               + [G.nodes[pattern[3]][node_label]] \n",
    "                               + [G[pattern[3]][pattern[0]][edge_label]]\n",
    "                               + [G.nodes[pattern[4]][node_label]] \n",
    "                               + [G[pattern[4]][pattern[3]][edge_label]])\n",
    "            treelet.append(canonkey_t)\n",
    "        canonkey_l.update(Counter(treelet))\n",
    "\n",
    "        # pattern 11\n",
    "        treelet = []\n",
    "        for pattern in patterns['11']:\n",
    "            canonlist = [tuple((G.nodes[leaf][node_label], \n",
    "                                G[leaf][pattern[0]][edge_label])) for leaf in pattern[1:4]]\n",
    "            canonlist.sort()\n",
    "            canonlist = list(chain.from_iterable(canonlist))\n",
    "            canonkey_t = tuple(['b'] + [G.nodes[pattern[0]][node_label]] + canonlist \n",
    "                               + [G.nodes[pattern[4]][node_label]] \n",
    "                               + [G[pattern[4]][pattern[0]][edge_label]]\n",
    "                               + [G.nodes[pattern[5]][node_label]] \n",
    "                               + [G[pattern[5]][pattern[4]][edge_label]])\n",
    "            treelet.append(canonkey_t)\n",
    "        canonkey_l.update(Counter(treelet))\n",
    "\n",
    "        # pattern 10\n",
    "        treelet = []\n",
    "        for pattern in patterns['10']:\n",
    "            canonkey4 = [G.nodes[pattern[5]][node_label], G[pattern[5]][pattern[4]][edge_label]]\n",
    "            canonlist = [tuple((G.nodes[leaf][node_label], \n",
    "                                G[leaf][pattern[0]][edge_label])) for leaf in pattern[1:3]]\n",
    "            canonlist.sort()\n",
    "            canonkey0 = list(chain.from_iterable(canonlist))\n",
    "            canonkey_t = tuple(['a'] + [G.nodes[pattern[3]][node_label]] \n",
    "                               + [G.nodes[pattern[4]][node_label]] \n",
    "                               + [G[pattern[4]][pattern[3]][edge_label]] \n",
    "                               + [G.nodes[pattern[0]][node_label]] \n",
    "                               + [G[pattern[0]][pattern[3]][edge_label]] \n",
    "                               + canonkey4 + canonkey0)\n",
    "            treelet.append(canonkey_t)\n",
    "        canonkey_l.update(Counter(treelet))\n",
    "\n",
    "        # pattern 12\n",
    "        treelet = []\n",
    "        for pattern in patterns['12']:\n",
    "            canonlist0 = [tuple((G.nodes[leaf][node_label], \n",
    "                                 G[leaf][pattern[0]][edge_label])) for leaf in pattern[1:3]]\n",
    "            canonlist0.sort()\n",
    "            canonlist0 = list(chain.from_iterable(canonlist0))\n",
    "            canonlist3 = [tuple((G.nodes[leaf][node_label], \n",
    "                                 G[leaf][pattern[3]][edge_label])) for leaf in pattern[4:6]]\n",
    "            canonlist3.sort()\n",
    "            canonlist3 = list(chain.from_iterable(canonlist3))\n",
    "            \n",
    "            # 2 possible key can be generated from 2 nodes with extended label 3, \n",
    "            # select the one with lower lexicographic order.\n",
    "            canonkey_t1 = tuple(['c'] + [G.nodes[pattern[0]][node_label]] + canonlist0 \n",
    "                                + [G.nodes[pattern[3]][node_label]] \n",
    "                                + [G[pattern[3]][pattern[0]][edge_label]] \n",
    "                                + canonlist3)\n",
    "            canonkey_t2 = tuple(['c'] + [G.nodes[pattern[3]][node_label]] + canonlist3 \n",
    "                                + [G.nodes[pattern[0]][node_label]] \n",
    "                                + [G[pattern[0]][pattern[3]][edge_label]] \n",
    "                                + canonlist0)\n",
    "            treelet.append(canonkey_t1 if canonkey_t1 < canonkey_t2 else canonkey_t2)\n",
    "        canonkey_l.update(Counter(treelet))\n",
    "\n",
    "        # pattern 9\n",
    "        treelet = []\n",
    "        for pattern in patterns['9']:\n",
    "            canonkey2 = [G.nodes[pattern[4]][node_label], G[pattern[4]][pattern[2]][edge_label]]\n",
    "            canonkey3 = [G.nodes[pattern[5]][node_label], G[pattern[5]][pattern[3]][edge_label]]\n",
    "            prekey2 = [G.nodes[pattern[2]][node_label], G[pattern[2]][pattern[0]][edge_label]]\n",
    "            prekey3 = [G.nodes[pattern[3]][node_label], G[pattern[3]][pattern[0]][edge_label]]\n",
    "            if prekey2 + canonkey2 < prekey3 + canonkey3:\n",
    "                canonkey_t = [G.nodes[pattern[1]][node_label]] \\\n",
    "                             + [G[pattern[1]][pattern[0]][edge_label]] \\\n",
    "                             + prekey2 + prekey3 + canonkey2 + canonkey3\n",
    "            else:\n",
    "                canonkey_t = [G.nodes[pattern[1]][node_label]] \\\n",
    "                             + [G[pattern[1]][pattern[0]][edge_label]] \\\n",
    "                             + prekey3 + prekey2 + canonkey3 + canonkey2\n",
    "            treelet.append(tuple(['9'] + [G.nodes[pattern[0]][node_label]] + canonkey_t))\n",
    "        canonkey_l.update(Counter(treelet))\n",
    "\n",
    "        return canonkey_l\n",
    "\n",
    "    return canonkey\n",
    "\n",
    "\n",
    "def wrapper_get_canonkeys(node_label, edge_label, labeled, is_directed, itr_item):\n",
    "    g = itr_item[0]\n",
    "    i = itr_item[1]\n",
    "    return i, get_canonkeys(g, node_label, edge_label, labeled, is_directed)\n",
    "    \n",
    "\n",
    "def find_paths(G, source_node, length):\n",
    "    \"\"\"Find all paths with a certain length those start from a source node. \n",
    "    A recursive depth first search is applied.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G : NetworkX graphs\n",
    "        The graph in which paths are searched.\n",
    "    source_node : integer\n",
    "        The number of the node from where all paths start.\n",
    "    length : integer\n",
    "        The length of paths.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    path : list of list\n",
    "        List of paths retrieved, where each path is represented by a list of nodes.\n",
    "    \"\"\"\n",
    "    if length == 0:\n",
    "        return [[source_node]]\n",
    "    path = [[source_node] + path for neighbor in G[source_node] \\\n",
    "        for path in find_paths(G, neighbor, length - 1) if source_node not in path]\n",
    "    return path\n",
    "\n",
    "\n",
    "def find_all_paths(G, length, is_directed):\n",
    "    \"\"\"Find all paths with a certain length in a graph. A recursive depth first\n",
    "    search is applied.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G : NetworkX graphs\n",
    "        The graph in which paths are searched.\n",
    "    length : integer\n",
    "        The length of paths.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    path : list of list\n",
    "        List of paths retrieved, where each path is represented by a list of nodes.\n",
    "    \"\"\"\n",
    "    all_paths = []\n",
    "    for node in G:\n",
    "        all_paths.extend(find_paths(G, node, length))\n",
    "        \n",
    "    if not is_directed:\n",
    "        # For each path, two presentations are retrieved from its two extremities. \n",
    "        # Remove one of them.\n",
    "        all_paths_r = [path[::-1] for path in all_paths]  \n",
    "        for idx, path in enumerate(all_paths[:-1]):\n",
    "            for path2 in all_paths_r[idx+1::]:\n",
    "                if path == path2:\n",
    "                    all_paths[idx] = []\n",
    "                    break\n",
    "        all_paths = list(filter(lambda a: a != [], all_paths))\n",
    "            \n",
    "    return all_paths\n",
    "\n",
    "##################################CODE AS IT IS ENDS HERE##################################################################\n",
    "\n",
    "#   compute_kernel_matrix(Kmatrix, all_num_of_each_label, Gn, parallel, n_jobs, False)\n",
    "\n",
    "#   return Kmatrix\n",
    "\n",
    "##################################CODE FOR TESTING BEGINS HERE#############################################################\n",
    "\n",
    "# dataset, y_all = loadDataset(dslist[z]['datafile'], filename_y=dslist[z]['dataset_y'], extra_params=None)\n",
    "# nx.draw_networkx(dataset[20])\n",
    "# print(\"##########DEBUG##########\")\n",
    "# print(np.array(list(dataset[20].nodes())).min())\n",
    "for z in range(len(dslist)):\n",
    "    dataset, y_all = loadDataset(dslist[z]['datafile'], filename_y=dslist[z]['dataset_y'], extra_params=None)\n",
    "\n",
    "    y_mid = np.array(y_all)\n",
    "\n",
    "    y_mid = (y_mid - y_mid.min())/(y_mid.max()-y_mid.min())\n",
    "    y_all = list(y_mid)\n",
    "#     print(y_all)\n",
    "#     nx.draw_networkx(dataset[0])\n",
    "\n",
    "    # print(dataset[50].nodes)\n",
    "\n",
    "    Kmatrix, time = weisfeilerlehmankernel(dataset[:], height = 10, numlandmarks =4)\n",
    "\n",
    "    # estimator = treeletkernel\n",
    "    param_grid_precomputed = {'sub_kernel': [gaussiankernel, polynomialkernel]}\n",
    "    param_grid = [{'C': np.logspace(-10, 10, num=41, base=10)},\n",
    "                  {'alpha': np.logspace(-10, 10, num=41, base=10)}]\n",
    "\n",
    "    param_list_precomputed = list(ParameterGrid(param_grid_precomputed))\n",
    "    param_list = list(ParameterGrid(param_grid))\n",
    "\n",
    "    Kmatrix_diag = Kmatrix.diagonal().copy()\n",
    "\n",
    "    # Removing 0 Diagonal elements\n",
    "#     nb_g_ignore = 0\n",
    "#     for idxk, diag in enumerate(Kmatrix_diag):\n",
    "#         if diag == 0:\n",
    "#             Kmatrix = np.delete(Kmatrix, (idxk - nb_g_ignore), axis=0)\n",
    "#             Kmatrix = np.delete(Kmatrix, (idxk - nb_g_ignore), axis=1)\n",
    "#             nb_g_ignore += 1\n",
    "\n",
    "    # Normalise the Kmatrix\n",
    "    for i in range (len(Kmatrix)):\n",
    "        for j in range(i, len(Kmatrix)):\n",
    "            if (Kmatrix_diag[i] == 0 or Kmatrix_diag[j] == 0):\n",
    "                Kmatrix[i][j] = 0\n",
    "            else:\n",
    "                Kmatrix[i][j] /= np.sqrt(Kmatrix_diag[i]*Kmatrix_diag[j])\n",
    "\n",
    "    gram_matrices = []\n",
    "    gram_matrices.append(Kmatrix)\n",
    "    y = y_all[:]\n",
    "\n",
    "    indices = range(len(y))\n",
    "    gm_now = gram_matrices[0].copy()\n",
    "\n",
    "    X_app, X_test, y_app, y_test, idx_app, idx_test = train_test_split(gm_now, y, indices, test_size=0.1, random_state=1, shuffle=True)\n",
    "\n",
    "    X_app = X_app[:, idx_app]\n",
    "    X_test = X_test[:, idx_app]\n",
    "    y_app = np.array(y_app)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    inner_cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "    current_train_perf = []\n",
    "    current_valid_perf = []\n",
    "    current_test_perf = [] \n",
    "    \n",
    "    if dslist[z]['task'] == 'regression':\n",
    "        print(\"regression being done:\\n\")\n",
    "        kr = SVR(kernel='precomputed')\n",
    "        for train_index, valid_index in inner_cv.split(X_app):\n",
    "            kr.fit(X_app[train_index, :][:, train_index],\n",
    "                   y_app[train_index])\n",
    "\n",
    "            # predict on the train, validation and test set\n",
    "            y_pred_train = kr.predict(\n",
    "                X_app[train_index, :][:, train_index])\n",
    "            y_pred_valid = kr.predict(\n",
    "                X_app[valid_index, :][:, train_index])\n",
    "        #                    if trial == 0:     \n",
    "        #                        print('y_pred_valid: ', y_pred_valid)\n",
    "        #                        print()\n",
    "            y_pred_test = kr.predict(\n",
    "                X_test[:, train_index])\n",
    "\n",
    "            # root mean squared errors\n",
    "        # #     print(\n",
    "        #              mean_squared_error(\n",
    "        #                 y_app[train_index], y_pred_train))\n",
    "            current_train_perf.append(\n",
    "                np.sqrt(\n",
    "                    mean_squared_error(\n",
    "                        y_app[train_index], y_pred_train)))\n",
    "            current_valid_perf.append(\n",
    "                np.sqrt(\n",
    "                    mean_squared_error(\n",
    "                        y_app[valid_index], y_pred_valid)))\n",
    "        #                    if trial == 0:\n",
    "        #                        print(mean_squared_error(\n",
    "        #                                y_app[valid_index], y_pred_valid))\n",
    "            current_test_perf.append(\n",
    "                np.sqrt(\n",
    "                    mean_squared_error(\n",
    "                        y_test, y_pred_test)))\n",
    "\n",
    "#         print(y_test)\n",
    "#         print(y_pred_test)\n",
    "        train_pref = np.mean(\n",
    "                        current_train_perf)\n",
    "        val_pref = np.mean(\n",
    "                        current_valid_perf)\n",
    "        test_pref = np.mean(\n",
    "                        current_test_perf)\n",
    "\n",
    "        print(train_pref, val_pref, test_pref)\n",
    "    else:\n",
    "        print(\"classification being done\\n\")\n",
    "        svc = SVC(kernel = 'precomputed')\n",
    "        for train_index, valid_index in inner_cv.split(X_app):\n",
    "            svc.fit(X_app[train_index, :][:, train_index],\n",
    "                   y_app[train_index])\n",
    "\n",
    "            # predict on the train, validation and test set\n",
    "            y_pred_train = svc.predict(\n",
    "                X_app[train_index, :][:, train_index])\n",
    "            y_pred_valid = svc.predict(\n",
    "                X_app[valid_index, :][:, train_index])\n",
    "            y_pred_test = svc.predict(\n",
    "                X_test[:, train_index])\n",
    "\n",
    "            # root mean squared errors\n",
    "            current_train_perf.append(\n",
    "                accuracy_score(y_app[train_index],\n",
    "                               y_pred_train))\n",
    "            current_valid_perf.append(\n",
    "                accuracy_score(y_app[valid_index],\n",
    "                               y_pred_valid))\n",
    "            current_test_perf.append(\n",
    "                accuracy_score(y_test, y_pred_test))\n",
    "            \n",
    "        train_pref = np.mean(\n",
    "            current_train_perf)\n",
    "        val_pref = np.mean(\n",
    "            current_valid_perf)\n",
    "        test_pref = np.mean(\n",
    "            current_test_perf)\n",
    "        print(train_pref, val_pref, test_pref)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [2 2]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[50,5],[2,6],[9,5.5]])\n",
    "print(np.argsort(a, axis = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
